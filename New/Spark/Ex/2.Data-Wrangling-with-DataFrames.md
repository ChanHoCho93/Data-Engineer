# Data Wrangling with DataFrames

## Data Wrangling with Spark

Data wrangling describes data processing activities like **cleansing, sorting, filtering**, and **grouping** data. PySpark provides numerous functions in the [DataFrame API ](https://spark.apache.org/docs/3.1.1/api/python/reference/pyspark.sql.html#dataframe-apis)for wrangling data. Using the same music streaming service log dataset, let's do some data wrangling using DataFrames

- Print the schema, so we can see the columns we have in the DataFrame
- Call `describe` on the whole frame to see the values of each column
- Check how many rows we have in the data frame with the `count` method
- Use `dropDuplicates` to see each kind once
- Filter out all rows with an empty string value as the user ID.
- Use a window function to compute cumulative sums

<iframe frameborder="0" allowfullscreen="1" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" title="YouTube video player" width="640" height="360" src="https://www.youtube.com/embed/p7YrwF72gQs?showinfo=0&amp;rel=0&amp;autohide=1&amp;vq=hd720&amp;hl=en-us&amp;cc_load_policy=0&amp;enablejsapi=1&amp;origin=https%3A%2F%2Flearn.udacity.com&amp;widgetid=49" id="widget50" style="border: 0px; box-sizing: border-box; overflow-wrap: break-word; outline-color: var(--chakra-colors-blue-500); display: block; top: 0px; bottom: 0px; left: 0px; width: 878px; height: 493.875px;"></iframe>

[Github link](https://github.com/udacity/nd027-Data-Engineering-Data-Lakes-AWS-Exercises/blob/main/lesson-2-spark-essentials/exercises/concept3-data-wrangling-with-spark/solution/data_wrangling.py) for source code

### Data Wrangling User Logs

This first section imports libraries, instantiates a `SparkSession`, and then reads in the data set

```
from pyspark.sql import SparkSession
from pyspark.sql.functions import udf
from pyspark.sql.types import StringType
from pyspark.sql.types import IntegerType
from pyspark.sql.functions import desc
from pyspark.sql.functions import asc
from pyspark.sql.functions import sum as Fsum

import datetime

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
spark = SparkSession \
    .builder \
    .appName("Wrangling Data") \
    .getOrCreate()
path = "../../data/sparkify_log_small.json"
user_log = spark.read.json(path)
```

### Data Exploration

Let's explore the data by inspecting the first 5 rows in the DataFrame with the `.take()` function

```
user_log.take(5)
```

Print the schema with `.printSchema()`

```
user_log.printSchema()
```

Which displays the schema including column names and data types:

```
root
 |-- artist: string (nullable = true)
 |-- auth: string (nullable = true)
 |-- firstName: string (nullable = true)
 |-- gender: string (nullable = true)
 |-- itemInSession: long (nullable = true)
 |-- lastName: string (nullable = true)
 |-- length: double (nullable = true)
 |-- level: string (nullable = true)
 |-- location: string (nullable = true)
 |-- method: string (nullable = true)
 |-- page: string (nullable = true)
 |-- registration: long (nullable = true)
 |-- sessionId: long (nullable = true)
 |-- song: string (nullable = true)
 |-- status: long (nullable = true)
 |-- ts: long (nullable = true)
 |-- userAgent: string (nullable = true)
 |-- userId: string (nullable = true)
```

Display a DataFrame in a table format with the .show() function

```
user_log.describe().show()
```

Use the [`.describe()`function](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.describe.html#pyspark.sql.DataFrame.describe) with the `.show()` function to explore basic statistics about the data.

```
user_log.describe("artist").show()
+-------+-----------------+
|summary|           artist|
+-------+-----------------+
|  count|             8347|
|   mean|            461.0|
| stddev|            300.0|
|    min|              !!!|
|    max|ÃÂlafur Arnalds|
+-------+-----------------+
    ```
    
    Use .dropDuplicates() to see each kind of page once, and sort by page
    
    ```undefined
user_log.select("page").dropDuplicates().sort("page").show()
```

Select data for all pages where userId is 1046

```
user_log_df.select(["userId", "firstname", "page", "song"]) .where(user_log_df.userId == "1046").show()
```

### Calculating Statistics by Hour

Display the results in a scatter plot

```
get_hour = udf(lambda x: datetime.datetime.fromtimestamp(x / 1000.0). hour)
user_log = user_log.withColumn("hour", get_hour(user_log.ts))
user_log.head()
songs_in_hour = user_log.filter(user_log.page == "NextSong").groupby(user_log.hour).count().orderBy(user_log.hour.cast("float"))
songs_in_hour.show()
songs_in_hour_pd = songs_in_hour.toPandas()
songs_in_hour_pd.hour = pd.to_numeric(songs_in_hour_pd.hour)
plt.scatter(songs_in_hour_pd["hour"], songs_in_hour_pd["count"])
plt.xlim(-1, 24);
plt.ylim(0, 1.2 * max(songs_in_hour_pd["count"]))
plt.xlabel("Hour")
plt.ylabel("Songs played");
```

### Drop Rows with Missing Values

As you'll see, there are no missing values in the userID or session columns. But there are userID values that are empty strings.

```
user_log_valid = user_log.dropna(how = "any", subset = ["userId", "sessionId"])
user_log_valid.count()
user_log.select("userId").dropDuplicates().sort("userId").show()
user_log_valid = user_log_valid.filter(user_log_valid["userId"] != "")
user_log_valid.count()
```

# Users Downgrade Their Accounts

Find when users downgrade their accounts and then flag those log entries. Then use a window function and cumulative sum to distinguish each user's data as either pre or post-downgrade events.

```
user_log_valid.filter("page = 'Submit Downgrade'").show()
user_log.select(["userId", "firstname", "page", "level", "song"]).where(user_log.userId == "1138").collect()
flag_downgrade_event = udf(lambda x: 1 if x == "Submit Downgrade" else 0, IntegerType())
user_log_valid = user_log_valid.withColumn("downgraded", flag_downgrade_event("page"))
user_log_valid.head()
from pyspark.sql import Window
windowval = Window.partitionBy("userId").orderBy(desc("ts")).rangeBetween(Window.unboundedPreceding, 0)
user_log_valid = user_log_valid.withColumn("phase", Fsum("downgraded").over(windowval))
user_log_valid.select(["userId", "firstname", "ts", "page", "level", "phase"]).where(user_log.userId == "1138").sort("ts").collect()
```

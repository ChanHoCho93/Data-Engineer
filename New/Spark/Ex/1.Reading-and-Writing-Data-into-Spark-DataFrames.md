# Exercise: Reading and Writing Data into Spark DataFrames

## Reading and Writing into Spark Data Frames

In the last Spark example, you saw how Spark can be used to transform data. Now let's look at how we can use Spark to wrangle and explore data using the [**DataFrame** API](https://spark.apache.org/docs/latest/sql-programming-guide.html). In this video, we'll import and export data to and from Spark DataFrames using a dataset that describes log events coming from a music streaming service.

In the screencast below, we will:

- Create a Spark session with parameters.
- Load a JSON file into a Spark DataFrame called `user_log`.
- Print the schema with the `printSchema()` method.
- Try the `describe()` method to see what we can learn from our data.
- Use the `take()` method to grab the first few records.
- Save it into a different format, for example, into a CSV file, with the `write.save()` method
- Use the read.csv method

<iframe frameborder="0" allowfullscreen="1" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" title="DEND C3 L03 A08 Demo- Reading And Writing Data Into DataFrame" width="640" height="360" src="https://www.youtube.com/embed/DR49NVl8HBM?showinfo=0&amp;rel=0&amp;autohide=1&amp;vq=hd720&amp;hl=en-us&amp;cc_load_policy=0&amp;enablejsapi=1&amp;origin=https%3A%2F%2Flearn.udacity.com&amp;widgetid=53" id="widget54" style="border: 0px; box-sizing: border-box; overflow-wrap: break-word; outline-color: var(--chakra-colors-blue-500); display: block; top: 0px; bottom: 0px; left: 0px; width: 878px; height: 493.875px;"></iframe>

### Reading Data into Dataframes

Let's create our first DataFrame from a fairly small sample data set. Throughout the lesson we'll work with a log file data set that describes user interactions with a music streaming service. The records describe events such as logging in to the site, visiting a page, listening to the next song, and seeing an ad.

Let's define a path to the data, in this example, we'll use data stored locally:

```
path = "../../data/sparkify_log_small.json"
```

Use `spark.read.json(path)` to read JSON data into Spark to work with it in a DataFrame:

```
user_log = spark.read.json(path)
```

Consult the [Spark documentation](https://spark.apache.org/docs/latest/sql-data-sources-load-save-functions.html) for a complete list of all the file formats and data sources Spark can read from. If a file is stored in cloud storage, you can use the same methods. When specifying the file path we just need to make sure that we are pointing to the cloud storage path that stores our target file.

### Writing Data from DataFrames

Spark can write data in DataFrames to many formats, in this example we'll write the DataFrame from the JSON file into a new CSV file, essentially converting JSON to CSV.

Set a path and name for the file to be written:

out_path = "../../data/sparkify_log_small.csv"

Use the DataFrame `object.write.save()` using options to specify the format and header

```
user_log.write.save(out_path, format="csv", header=True)
```

### Tip

If Spark is used in a cluster mode all the worker nodes need to have access to the input data source. If you're trying to import a file saved only on the local disk of the driver node you'll receive an error message similar to this:

```
AnalysisException: u'Path does not exist: file:/home/ubuntu/test.csv;'
```

Loading the file should work if all the nodes have it saved under the same path.

### Documentation

You can find the documentation for Spark read and write operations [here.](https://spark.apache.org/docs/latest/sql-data-sources-load-save-functions.html)

### Need a Refresher on DataFrames?

The Python Pandas library provides [documentation](https://pandas.pydata.org/docs/getting_started/intro_tutorials/01_table_oriented.html) if you'd like to review DataFrame concepts.
